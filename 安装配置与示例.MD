## 在windows安装Spark环境，所需依赖为jdk/Spark/Hadoop

1. 安装jdk，http://www.oracle.com/technetwork/java/javase/downloads/index.html
安装完成后配置环境变量，过程略

2. 安装Scala，http://www.scala-lang.org/
选择windows版本，scala-2.12.3.msi
安装完成之后，先设置环境变量，将scala安装目录的bin目录添加到path系统变量中，然后使用scala命令验证安装结果

3. 安装Spark，http://spark.apache.org/
此处我们选择Spark-2.2.0-bin-hadoop2.7.tgz，解压后添加SPARK_HOME环境变量，变量值为spark安装目录；将Spark/bin的所在目录添加到path系统变量中，完成之后，此时如果用spark-shell命令验证安装是否成功会报错，
是因为还缺少了hadoop组件

4. 安装Hadoop，https://archive.apache.org/dist/hadoop/common/
选择的hadoop版本需要和第3步的spark匹配，所以此处选择hadoop2.7.0，下载解压后添加HAOOP_HOME环境变量，变量值为hadoop安装目录；将hadoop/bin目录添加到path系统环境变量中

上述完成之后使用spark-shell命令测试spark/hadoop是否安装成功，过程中可能会遇到的问题：  
* spark-shell命令报空指针问题  
主要是因为Hadoop的bin目录下没有winutils.exe文件的原因造成的。这里的解决办法是：可以去 https://github.com/steveloughran/winutils 选择你安装的Hadoop版本号，然后进入到bin目录下，找到winutils.exe文件，下载方法是点击winutils.exe文件，进入之后在页面的右上方部分有一个Download按钮，点击下载即可
* 出现的ava.net.BindException: Cannot assign requested address: Service 'sparkDriver' failed after 16 retries (starting from 0)!问题  
解决办法：https://blog.csdn.net/chengyuqiang/article/details/69665878 ，
hosts文件中添加 <本机ip> sk1

## 最后如何在python下搭建Spark环境：
1. 添加环境变量  
PATHONPATH=%SPARK_HOME%\python;%SPARK_HOME%\python\lib\py4j-0.10.8.1-src.zip
2. 安装pip install py4j

3. 安装pip install pyspark

4. 示例代码
    ```
      from pyspark import SparkConf, SparkContext
      conf = SparkConf().setMaster("local").setAppName("My App")
      '''可能遇到的问题：
         Error: Could not find valid SPARK_HOME while searching: 
         问题原因是，读取不到环境变量SPARK_HOME，请检查环境变量是否设置了SPARK_HOME
      '''
      sc = SparkContext(conf=conf)
      rdd = sc.parallelize(list(range(100)))
      print(rdd.collect())
    ```
    
    说明：rdd提供了非常丰富的算子Action，总体可以分为2类，
    * 转换
    在Spark中，转换的作用是从现有数据集创建新数据集。转换是惰性的，因为它们仅在动作需要将结果返回到驱动程序时才计算。
    * 行动
    在Spark中，操作的作用是在对数据集运行计算后将值返回给驱动程序。
    
    也就是说，如果只是转换算子在没有明确要求执行前只是记录了运算过程，并不会立即执行，直到执行了行动类的方法(如collect())后才会立即执行；
    而运算中一旦加入了行动算子，计算过程将立即被执行。
    rdd和pydash很相似，可以把pydash看作是单机计算，spark rdd是分布式计算。
